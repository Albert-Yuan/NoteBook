#spark
Spark 是一个用来实现快速而通用的集群计算的平台

批处理、迭代算法、交互式查询、流处理

提供基于 Python、 Java、 Scala 和 SQL 的简单易用的API 以及内建程序库

shell输出的日志
在 conf 目录下创建一个名为 log4j.properties 的文件来管理日志设置
日志设置文件的模版，叫作 log4j.properties.template

IPython是一个增强版Python shell

弹性分布式数据集（ resilient distributed dataset），简称 RDD

sc.textFile() 来创建一个代表文件中各行文本的RDD //SparkContext

用户可以使用两种方法创建 RDD： 读取一个外部数据集，或在驱动器程序里分发驱动器程序中的对象集合（ 比如 list 和 set）。

默认情况下，Spark的RDD会在你每次对它们进行行动操作时重新计算
如果想在多个行动操作中重用同一个RDD可以使用RDD.persist()把RDD缓存
cache()与使用默认存储级别调用persist()是一样的
RDD的转化操作都是惰性求值的，在被调用行动操作之前Spark不会开始计算

两个最常用的转化操作是 map() 和 filter()

































































































































































































